---
permalink: /blog/cache.html
layout: post
title: "Cache Me If You Can"
tags: cache architecture about-us
---

## Speed Is Important

Speed is the primary factor in determining customers' satisfaction with search results. If searchers think that a results page isn't loading quickly enough, they'll hit the back button and abandon their search before they even see if the results are relevant.

Google's [Gospel of Speed](https://www.google.com/search?q=Google+Gospel+of+Speed) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) motto laid the foundation for the company's success. The need for speed is now extending beyond search and into all web sites and applications as [web users expect results in the blink of an eye](http://www.nytimes.com/2012/03/01/technology/impatient-web-users-flee-slow-loading-sites.html) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif). 

## We Focus on Speed

When we established our service's new open source architecture in 2010, we first tackled how to deliver our results in under one second. At around the same time, [Github](http://www.github.com) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) was experiencing exponential growth and the company's engineers were blogging about what they did to make Github fast. To get up to speed quickly (yes, bad pun intended), we read their posts. 

* [How We Made GitHub Fast](https://github.com/blog/530-how-we-made-github-fast) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif), October 20, 2009

* [How We Keep GitHub Fast](https://github.com/blog/1252-how-we-keep-github-fast) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif), September 5, 2012

Leveraging some of their tips, we architected a service that succeeded in delivering our results in under 700 milliseconds, on average&mdash;a significant improvement from the previous vendor-owned and -operated iterations of our service. 

Over the past three years, we've dug in and improved our response time. We now deliver our results in under 500 milliseconds, on average. 

We already had an architecture optimized for speed. So, how did we speed it up by 200 milliseconds?

## We Use Cache to Lower Latency

In a nutshell, speed = low latency + high bandwidth.

[Latency](http://www.igvita.com/2012/07/19/latency-the-new-web-performance-bottleneck/) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) is delay. It is the amount of time it takes a packet of information to travel from our servers to a searcher's web browser. In our quest to shave precious milliseconds off of our response time, we've spent much of our time focused on the latency side of the equation. 

Caching is a key strategy that we use to lower our latency. Within each logical component of our system (as shown in the diagram below), we ask ourselves, "How we can make *this* faster?"

![Logical system components](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/cache.png)

### Optimizing Our Software Stack

[Resque](https://github.com/resque) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) serves as our first line of defense to help us reduce our latency. Resque is a [Redis](http://redis.io/)-backed  ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) Ruby library that allows us to store our various objects and datasets without searchers ever having to access our system's components. Read [*Introducing Resque*](https://github.com/blog/542-introducing-resque) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) to learn more about how it works.

Moving down through our software stack, we also monitor and preprocess popular requests to our [MySQL](http://www.mysql.com/) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) database. For example, most of our agency customers view their [Monthly Report](/sites/manual/monthly-report.html) within the first fews days of the month. Generating these reports "on the fly" during this relatively short window could strain our system. So, instead, we generate static monthly reports outside of core business hours. This preprocessing speeds up searchers' experience on our results pages plus it saves us money because we don't need extra servers to meet the burst in demand.

### Using a Content Delivery Network

Our static content (such as javascripts and sytlesheets) gets served through our [content delivery network](http://www.webopedia.com/TERM/C/CDN.html) ![External link](https://9fddeb862c037f6d2190-f1564c64756a8cfee25b6b19953b1d23.ssl.cf2.rackcdn.com/external_link.gif) provider, currently Akamai. Akamai serves our static content from its server that is geographically closest to the searcher. The closer, the faster.

Using a content delivery network also allow us to optimize our service's speed by:

* Directing non-cached traffic between our two datacenters to create a [multihomed environment](/releases/2012-10-19.html). Multihoming allows us to make full use of all of our servers. By contrast, in 2010, our disaster recovery datacenter often sat idle. 
* Reducing our need to add bandwidth or servers to handle short-term traffic spurts, such as spurts related to natural disasters.
* Protecting against denial of service attacks by spotting them before they reach our servers.

## What's Next?

While the 10,000 foot view of our architecture today looks similar to our architecture in 2010, we've worked hard over the past three years to speed up the delivery of our results by optimizing each link in the chain. 

We use several monitoring tools to measure our system's performance. The quality of these tools is improving at a rapid pace, which in turn, shows us where and how we can improve our service.

We regularly ask ourselves, "Will *this* shave some time off and help us deliver our results in *under* 500 milliseconds?"